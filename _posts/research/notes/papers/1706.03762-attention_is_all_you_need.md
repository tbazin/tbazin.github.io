# Attention is all you need

## Summary

* Introduces **Transformer**, an encoder-decoder architecture for sequence
  translation built _solely around attention modules_.
    - In particular, it is **neither convolutional, nor recurrent**.
* The test performance are _state-of-the-art with a reduced training cost_.
* Two types of attention modules are used:
    - Encoder-Decoder Attention,
    - Self-attention.
* The Attention algorithm is a _scaled variant of multiplicative, dot-product
  Attention_.

## Attention modules

<p align="center">
<img src="http://imgur.com/1krF2R6.png" width="250" title="Transformer architecture" alt="Transformer architecture">
</p>

### Scaled Dot-Product Attention
<p align="center">
<img src="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/MultiHead.png" width="300" title="Multi-Head Attention" alt="Multi-Head Attention">
</p>

* With $Q, K, V$ respectively the queries, keys and values in the attention
module:
    - $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$
* The **scaling factor** $\sqrt{d_k}$ helps **counteract the effect of increasing
  the dimension of the keys**:
    - With increasing $d_k$, the dot-product $QK^T$ has _higher variance_
        - e.g. The dot product of two independent random vectors of dimension
        $d$ with mean $0$ and variance $1$ has mean $0$ and variance $d$
    - This leads to likelier saturation in the softmax and less effective
    gradient steps.

### Multi-Head Attention

* The authors observe that multi-head attention (with $h = 8$ heads) gives
  better results than single head attention.
* They use reduced dimensions of $d_k / h$ for the keys in the multi-head case,
  thus retaining a complexity on par with the single head case.

### Self-Attention

* Both the encoder and the decoder use self-attention module, which allow them
  to attend (in a single sequential step) to all other input/output elements.

[Transformer architecture]: https://camo.githubusercontent.com/88e8f36ce61dedfd2491885b8df2f68c4d1f92f5/687474703a2f2f696d6775722e636f6d2f316b72463252362e706e67&f=1
[Scaled Attention]: https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/MultiHead.png
