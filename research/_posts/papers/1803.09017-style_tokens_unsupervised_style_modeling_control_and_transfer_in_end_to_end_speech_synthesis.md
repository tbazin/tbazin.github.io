# Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis

## Summary

* _Context_: End-to-End TTS + Style modeling
* _Main contribution_
    - GST latent vectors (Global Style Tokens)
    - These are embedding vectors extracted from the audio and trained jointly
    with the TTS model
    - They can be applied at inference time to control prosody style as well as
    apply the style of an external, previously unseen speaker -- extracted from
    an audio reference extract.

![Image][Model architecture]

## Key ideas

### GSTs

* The GSTs are trained as an attention module over an embedding of the audio
content (the "_reference vector_").

#### Reference vectors

* The _reference vectors_ ([Skerry-Ryan et al., 2018]) are computed as the
output of a network composed of a stack of 6 convolutional layers followed
by an RNN

#### Attention module

* The Attention module uses _additive_ (i.e. weighted sum, i.e. softmax)
  attention over the trained GSTs to yield the _style embedding_.
  > It acts more as a similarity measure between the reference vector and the GSTs
  rather than as an alignment computation.

    * It is recommended to use multi-head attention ([Vaswani et al., 2017]):
        - Split the input into $h$ zones and perform attention in parallel over
        each zone independently, then concatenate each zone-attention map to obtain
        the full attention map.
        - Can for instance use a $dim_{zoneEmbedding} = dim_{embedding} / h$ to
        operate at no computational overhead.

#### Style embedding

* The _style embedding_ (the output of the GST attention module) is used
during TTS by conditioning the RNN text **encoder** at each time-step with a
replicated version of it.
    - Note that it is the encoder that is being conditioned and **not the decoder**!
    This is shown to perform better experimentally.

### Inference

Two modes of inference:

* Manually (or by sampling, for random style sampling and browsing the style
space) set the style embedding as a chosen weighted-sum of the GSTs
* Use the reference vector extraction network + trained style token layer to
extract style embeddings from external, unseen audio.

#### Style morphing

* Whereas as train time the style embedding vector is replicated throughout the
text encoder, at inference time this is not required and one can vary it during
the course of synthesis, to have the speaking style vary over time.  


[Model architecture]: https://2.bp.blogspot.com/-Qq7kYTllplg/Wrp2uVX_vlI/AAAAAAAAChI/QF3QgeeDm7IgxlXmXvM4yE4MCTVwnQMuQCLcBGAs/s640/image2.png
[Skerry-Ryan et al., 2018]: https://arxiv.org/abs/1803.09047
[Vaswani et al., 2017]: https://arxiv.org/abs/1706.03762
